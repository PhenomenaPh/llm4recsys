{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0484188e",
   "metadata": {},
   "source": [
    "# Data and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6a94d5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:02:10.279846Z",
     "start_time": "2025-05-16T13:02:10.276656Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import html\n",
    "import re\n",
    "from tqdm.auto import tqdm  # Added tqdm import\n",
    "from transformers.pipelines import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer, KeyphraseTfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch, platform\n",
    "import random\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "import pymorphy3\n",
    "import spacy\n",
    "\n",
    "# Path\n",
    "PATH_TO_ITEMS = Path().cwd().parent / \"data\" / \"modified_data\" / \"items.parquet\"\n",
    "PATH_TO_NEW_ITEMS = Path().cwd().parent / \"data\" / \"modified_data\" / \"new_items.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5f7f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS:    PyTorch: 2.7.0+cu126\n",
      "MPS available: False\n",
      "MPS built‑in: False\n",
      "spaCy GPU usage: Enabled\n"
     ]
    }
   ],
   "source": [
    "# For MacOs to check if MPS is available\n",
    "\n",
    "print(\"macOS:\", platform.mac_ver()[0], \"  PyTorch:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built‑in:\", torch.backends.mps.is_built())\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "print(f\"spaCy GPU usage: {'Enabled' if spacy.require_gpu() else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f42a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifing seed value for reproducibility\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pl.set_random_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0bd854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:02:12.361094Z",
     "start_time": "2025-05-16T13:02:12.250343Z"
    }
   },
   "outputs": [],
   "source": [
    "items_df = pl.read_parquet(PATH_TO_ITEMS)\n",
    "\n",
    "# Checking if there is any null value in the DataFrame\n",
    "null_sum = items_df.null_count().sum_horizontal()[0]\n",
    "\n",
    "assert null_sum == 0, f\"There are {null_sum} null values in the DataFrame\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f0e77",
   "metadata": {},
   "source": [
    "# Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3e9be",
   "metadata": {},
   "source": [
    "## Keywords problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44232352",
   "metadata": {},
   "source": [
    "- Проверим, насколько наши текущие ключевые слова подходят для кластеризации\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e02bd1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: с прицепом\n",
      "Countries: сша\n",
      "Keywords: прицепом, 2017, США...\n",
      "--------------------------------------------------------------------------------\n",
      "Title: виктория\n",
      "Countries: великобритания\n",
      "Keywords: Виктория, 2016, Великобритания, брак, короли, королевы, коррупция, отцы, дети, политика, свадьбы, семейные, проблемы, семья, отношения, отношения, муж...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print samples of dataset's keywords\n",
    "sample_dataset = (\n",
    "    items_df.sample(2)\n",
    "    .select(\"keywords\", \"countries\", \"title\", \"description\")\n",
    "    .rows(named=True)\n",
    ")\n",
    "\n",
    "for item in sample_dataset:\n",
    "    print(f\"Title: {item['title']}\")\n",
    "    print(f\"Countries: {item['countries']}\")\n",
    "    print(f\"Keywords: {item['keywords'][:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3c985",
   "metadata": {},
   "source": [
    "- Во-первых, идет дубляция страны, что не хорошо. Зачем нам заниматься увеличением токенов, которые у нас повторяются в графе Стран\n",
    "- Видно, что предложения по типу \"борьба за выживание\" разделены запятой, а не точкой с запятой\n",
    "\n",
    "- Необходимо придумать более осмысленные ключевые слова!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211304b",
   "metadata": {},
   "source": [
    "### Bert for keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "777de83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# If mac, i am using Mac for this example\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# If you have gpu uncomment the line below\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\"cointegrated/LaBSE-en-ru\", device=device)\n",
    "\n",
    "kw_model = KeyBERT(model)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d824a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 151 Russian stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "print(f\"Loaded {len(russian_stopwords)} Russian stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa6f799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclean(text: str) -> str:\n",
    "    text = html.unescape(text)  # &amp; → &\n",
    "    text = unicodedata.normalize(\"NFKC\", text)  # длинные тире → обычные\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)  # убираем HTML\n",
    "    text = re.sub(r\"\\d{4}\", \" \", text)  # опц.: убираем года\n",
    "    text = re.sub(r\"[^\\S\\n]+\", \" \", text)  # множественные пробелы\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_cleaned_kw(text: str) -> str:\n",
    "    clean_text = preclean(text)\n",
    "\n",
    "    extracted_keywords_with_scores = kw_model.extract_keywords(\n",
    "        clean_text,  # Corrected from 'text' to 'clean_text'\n",
    "        use_mmr=True,\n",
    "        use_maxsum=True,\n",
    "        top_n=8,\n",
    "        threshold=0.35,\n",
    "        stop_words=russian_stopwords,\n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "    )\n",
    "\n",
    "    keyword_phrases = [phrase for phrase, score in extracted_keywords_with_scores]\n",
    "\n",
    "    return \", \".join(keyword_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eec350d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[43msample_dataset\u001b[49m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcurrent keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem[\u001b[33m'\u001b[39m\u001b[33mkeywords\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNew keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextract_cleaned_kw(item[\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'sample_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "for item in sample_dataset:\n",
    "    print(f\"current keywords: {item['keywords']}\")\n",
    "    print(f\"New keywords: {extract_cleaned_kw(item['description'])}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70bef6",
   "metadata": {},
   "source": [
    "- Перепробовал кучу способов, ничего лучше не смог найти.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35744531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Keywords: 15963it [28:29,  9.34it/s]                           \n",
      "Extracting Keywords: 15963it [28:29,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Calculate the number of items that will be processed by extract_cleaned_kw\n",
    "num_items_to_process = items_df.filter(pl.col(\"description\") != \"-\").height\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=num_items_to_process, desc=\"Extracting Keywords\")\n",
    "\n",
    "\n",
    "# Define a wrapper function that calls the original extract_cleaned_kw and updates the progress bar\n",
    "def extract_cleaned_kw_with_pbar_update(text: str) -> str:\n",
    "    result = extract_cleaned_kw(text)  # Call the original function\n",
    "    pbar.update(1)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Apply the transformation\n",
    "items_df = items_df.with_columns(\n",
    "    pl.when(pl.col(\"description\") != \"-\")\n",
    "    .then(\n",
    "        pl.col(\"description\").map_elements(\n",
    "            extract_cleaned_kw_with_pbar_update, return_dtype=pl.Utf8\n",
    "        )\n",
    "    )\n",
    "    .otherwise(pl.col(\"keywords\"))\n",
    "    .alias(\"new_keywords\")\n",
    ")\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406db7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'child'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mPATH_TO_ITEMS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchild\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PosixPath' object has no attribute 'child'"
     ]
    }
   ],
   "source": [
    "items_df.to_parquet(\n",
    "    PATH_TO_NEW_ITEMS,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
