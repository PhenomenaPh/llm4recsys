{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0484188e",
   "metadata": {},
   "source": [
    "# Data and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a94d5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:02:10.279846Z",
     "start_time": "2025-05-16T13:02:10.276656Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import html\n",
    "import re\n",
    "from tqdm.auto import tqdm  # Added tqdm import\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import numpy as np\n",
    "from keybert import KeyBERT\n",
    "import pymorphy3\n",
    "import spacy\n",
    "\n",
    "# Path\n",
    "PATH_TO_ITEMS = Path().cwd().parent / \"data\" / \"modified_data\" / \"items.parquet\"\n",
    "PATH_TO_NEW_ITEMS = Path().cwd().parent / \"data\" / \"modified_data\" / \"new_items.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8661898b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa5f7f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS: 15.4.1   PyTorch: 2.7.0\n",
      "MPS available: True\n",
      "MPS built‑in: True\n",
      "spaCy GPU usage: Enabled\n"
     ]
    }
   ],
   "source": [
    "# For MacOs to check if MPS is available\n",
    "\n",
    "print(\"macOS:\", platform.mac_ver()[0], \"  PyTorch:\", torch.__version__)\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built‑in:\", torch.backends.mps.is_built())\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "print(f\"spaCy GPU usage: {'Enabled' if spacy.require_gpu() else 'Disabled'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f42a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifing seed value for reproducibility\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "pl.set_random_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0bd854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-16T13:02:12.361094Z",
     "start_time": "2025-05-16T13:02:12.250343Z"
    }
   },
   "outputs": [],
   "source": [
    "items_df = pl.read_parquet(PATH_TO_ITEMS)\n",
    "\n",
    "# Checking if there is any null value in the DataFrame\n",
    "null_sum = items_df.null_count().sum_horizontal()[0]\n",
    "\n",
    "assert null_sum == 0, f\"There are {null_sum} null values in the DataFrame\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f0e77",
   "metadata": {},
   "source": [
    "# Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a3e9be",
   "metadata": {},
   "source": [
    "## Keywords problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44232352",
   "metadata": {},
   "source": [
    "- Проверим, насколько наши текущие ключевые слова подходят для кластеризации\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02bd1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: с прицепом\n",
      "Countries: сша\n",
      "Keywords: прицепом, 2017, США...\n",
      "--------------------------------------------------------------------------------\n",
      "Title: виктория\n",
      "Countries: великобритания\n",
      "Keywords: Виктория, 2016, Великобритания, брак, короли, королевы, коррупция, отцы, дети, политика, свадьбы, семейные, проблемы, семья, отношения, отношения, муж...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print samples of dataset's keywords\n",
    "sample_dataset = (\n",
    "    items_df.sample(2)\n",
    "    .select(\"keywords\", \"countries\", \"title\", \"description\")\n",
    "    .rows(named=True)\n",
    ")\n",
    "\n",
    "for item in sample_dataset:\n",
    "    print(f\"Title: {item['title']}\")\n",
    "    print(f\"Countries: {item['countries']}\")\n",
    "    print(f\"Keywords: {item['keywords'][:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db3c985",
   "metadata": {},
   "source": [
    "- Во-первых, идет дубляция страны, что не хорошо. Зачем нам заниматься увеличением токенов, которые у нас повторяются в графе Стран\n",
    "- Видно, что предложения по типу \"борьба за выживание\" разделены запятой, а не точкой с запятой\n",
    "\n",
    "- Необходимо придумать более осмысленные ключевые слова!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d211304b",
   "metadata": {},
   "source": [
    "### Bert for keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777de83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "# If mac, i am using Mac for this example\n",
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# If you have gpu uncomment the line below\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = SentenceTransformer(\"cointegrated/LaBSE-en-ru\", device=device)\n",
    "kw_model = KeyBERT(model)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d824a478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 151 Russian stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rustemhutiev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "print(f\"Loaded {len(russian_stopwords)} Russian stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa6f799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preclean(text: str) -> str:\n",
    "    text = html.unescape(text)  # &amp; → &\n",
    "    text = unicodedata.normalize(\"NFKC\", text)  # длинные тире → обычные\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)  # убираем HTML\n",
    "    text = re.sub(r\"\\d{4}\", \" \", text)  # опц.: убираем года\n",
    "    text = re.sub(r\"[^\\S\\n]+\", \" \", text)  # множественные пробелы\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_cleaned_kw(text: str) -> str:\n",
    "    clean_text = preclean(text)\n",
    "\n",
    "    extracted_keywords_with_scores = kw_model.extract_keywords(\n",
    "        clean_text,  # Corrected from 'text' to 'clean_text'\n",
    "        use_mmr=True,\n",
    "        use_maxsum=True,\n",
    "        top_n=8,\n",
    "        threshold=0.35,\n",
    "        stop_words=russian_stopwords,\n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "    )\n",
    "\n",
    "    keyword_phrases = [phrase for phrase, score in extracted_keywords_with_scores]\n",
    "\n",
    "    return \", \".join(keyword_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eec350d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current keywords: прицепом, 2017, США\n",
      "New keywords: трудности радости, актрисы фрэнки, мистер робот, молодая мама, сюжета двадцатилетняя, сериал прицепом, роль, основан личной\n",
      "--------------------------------------------------------------------------------\n",
      "current keywords: Виктория, 2016, Великобритания, брак, короли, королевы, коррупция, отцы, дети, политика, свадьбы, семейные, проблемы, семья, отношения, отношения, мужчины, женщины, отношения, мужа, жены, политический, конфликт, романтические, отношения, семейный, конфликт, политические, лидеры, исторические, события\n",
      "New keywords: британской королевы, коулман звезда, помощником викторию, периодов существования, сериал узнать, классическим байопиком, 18, окунулась\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in sample_dataset:\n",
    "    print(f\"current keywords: {item['keywords']}\")\n",
    "    print(f\"New keywords: {extract_cleaned_kw(item['description'])}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70bef6",
   "metadata": {},
   "source": [
    "- Перепробовал кучу способов, ничего лучше не смог найти.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35744531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Keywords: 15963it [28:29,  9.34it/s]                           \n",
      "Extracting Keywords: 15963it [28:29,  9.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of items that will be processed by extract_cleaned_kw\n",
    "num_items_to_process = items_df.filter(pl.col(\"description\") != \"-\").height\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=num_items_to_process, desc=\"Extracting Keywords\")\n",
    "\n",
    "\n",
    "# Define a wrapper function that calls the original extract_cleaned_kw and updates the progress bar\n",
    "def extract_cleaned_kw_with_pbar_update(text: str) -> str:\n",
    "    result = extract_cleaned_kw(text)  # Call the original function\n",
    "    pbar.update(1)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Apply the transformation\n",
    "items_df = items_df.with_columns(\n",
    "    pl.when(pl.col(\"description\") != \"-\")\n",
    "    .then(\n",
    "        pl.col(\"description\").map_elements(\n",
    "            extract_cleaned_kw_with_pbar_update, return_dtype=pl.Utf8\n",
    "        )\n",
    "    )\n",
    "    .otherwise(pl.col(\"keywords\"))\n",
    "    .alias(\"new_keywords\")\n",
    ")\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406db7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PosixPath' object has no attribute 'child'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mPATH_TO_ITEMS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchild\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: 'PosixPath' object has no attribute 'child'"
     ]
    }
   ],
   "source": [
    "# Saving our new dataset with keywords\n",
    "items_df.to_parquet(\n",
    "    PATH_TO_NEW_ITEMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab52959c",
   "metadata": {},
   "source": [
    "## Making General Description of an Item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9521bc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to item\n",
    "NEW_ITEMS_PATH = Path.cwd().parent / \"data\" / \"modified_data\" / \"new_items.parquet\"\n",
    "\n",
    "\n",
    "df_with_keywords = items_with_new_keywords = pl.read_parquet(NEW_ITEMS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6b21333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Unknown\" in df_with_keywords[\"release_year_range\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd971e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_description = df_with_keywords.with_columns(\n",
    "    pl.concat_str(\n",
    "        [\n",
    "            # Title section\n",
    "            pl.when(pl.col(\"title\").is_not_null())\n",
    "            .then(pl.lit(\"title: \") + pl.col(\"title\"))\n",
    "            .otherwise(None),\n",
    "            # Genres section\n",
    "            pl.when(pl.col(\"genres\").is_not_null())\n",
    "            .then(pl.lit(\"genres: \") + pl.col(\"genres\"))\n",
    "            .otherwise(None),\n",
    "            # Keywords section\n",
    "            pl.when(pl.col(\"new_keywords\") != \"unknown\")\n",
    "            .then(pl.lit(\"kw: \") + pl.col(\"new_keywords\"))\n",
    "            .otherwise(None),\n",
    "            # Plot/Description section\n",
    "            pl.when(pl.col(\"description\") != \"-\")\n",
    "            .then(pl.lit(\"plot: \") + pl.col(\"description\"))\n",
    "            .otherwise(None),\n",
    "            # Directors section\n",
    "            pl.when(pl.col(\"directors\") != \"unknown\")\n",
    "            .then(pl.lit(\"directors: \") + pl.col(\"directors\"))\n",
    "            .otherwise(None),\n",
    "            # Actors section\n",
    "            pl.when(pl.col(\"actors\") != \"unknown\")\n",
    "            .then(pl.lit(\"actors: \") + pl.col(\"actors\"))\n",
    "            .otherwise(None),\n",
    "            # Studios section\n",
    "            pl.when(pl.col(\"studios\") != \"unknown\")\n",
    "            .then(pl.lit(\"studios: \") + pl.col(\"studios\"))\n",
    "            .otherwise(None),\n",
    "            # Release year section\n",
    "            pl.when(pl.col(\"release_year_range\").is_not_null())\n",
    "            .then(pl.lit(\"release_year: \") + pl.col(\"release_year_range\"))\n",
    "            .otherwise(None),\n",
    "            # Age rating section\n",
    "            pl.when(pl.col(\"age_rating\").is_not_null())\n",
    "            .then(pl.lit(\"age_rating: \") + pl.col(\"age_rating\"))\n",
    "            .otherwise(None),\n",
    "        ],\n",
    "        separator=\"\\n\",\n",
    "        ignore_nulls=True,\n",
    "    ).alias(\"embedding_text\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c88b85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: тактическая сила\n",
      "genres: криминал, зарубежные, триллеры, боевики, комедии\n",
      "kw: капитан тейт, уайт темный, жесткую школу, навыки практике, онлайн тактическая, лишние свидетели, бандитские группировки, становятся\n",
      "plot: Профессиональный рестлер Стив Остин («Все или ничего») и темнокожий мачо Майкл Джей Уайт («Темный рыцарь») в интригующем криминальном боевике. В центре сюжета – команда спецназовцев, которая оказалась между двумя воюющими бандитскими группировками…  Говорят, что в спецназе нет более жестокого учителя и более профессионального бойца, чем капитан Тейт. Он заставляет своих рекрутов пройти жесткую школу, но зато после него они становятся высококлассными бойцами, которым все по плечу. Команда капитана прибывает для учений в заброшенный самолетный амбар, находящийся недалеко от города и давно превращенный в место для штатных учений. Однако вскоре оказывается, что именно здесь устраивают кровавые разборки две враждующие бандитские группировки. Лишние свидетели им ни к чему, поэтому они решают объединить усилия, чтобы без шума разделаться с новоприбывшими спецназовцами. Теперь капитану Тейту и его ученикам придется применить свои навыки на практике. Удастся ли им это, вы узнаете, если решите смотреть онлайн «Тактическая сила».\n",
      "directors: адам п. калтраро\n",
      "actors: Адриан Холмс, Даррен Шалави, Джерри Вассерман, Дэн Риззуто, Кендес Илэйн Калтраро, Кит Джардин, Лекса Дойг, Майкл Джей Уайт, Майкл Шэнкс, Майкл Эклунд, Питер Брайант, Питер Кент, Стив Бачич, Стив Остин\n",
      "release_year: 2010-2020\n",
      "age_rating: 16.0\n"
     ]
    }
   ],
   "source": [
    "print(df_with_description[\"embedding_text\"][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c0fdb",
   "metadata": {},
   "source": [
    "- Выглядит неплохо, однако все равно не нравятся ключевые слова\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b26849",
   "metadata": {},
   "source": [
    "# Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f38e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"cointegrated/LaBSE-en-ru\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3c62084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1104638a19473499b21b86ac984d84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenize:   0%|          | 0/15963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "texts = df_with_description[\"embedding_text\"].to_list()\n",
    "lens = []\n",
    "for txt in tqdm(texts, desc=\"tokenize\"):\n",
    "    lens.append(len(tokenizer.tokenize(txt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6d5bb087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg len: 278.7559982459438\n",
      "Median len: 242\n",
      "95th percentile: 487.0\n",
      "99th percentile: 550.0\n",
      "Max len: 2714\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, median\n",
    "\n",
    "avg_len = mean(lens)\n",
    "med_len = median(lens)\n",
    "p95 = np.percentile(lens, 95)\n",
    "p99 = np.percentile(lens, 99)\n",
    "max_len = max(lens)\n",
    "\n",
    "print(f\"Avg len: {avg_len}\")\n",
    "print(f\"Median len: {med_len}\")\n",
    "print(f\"95th percentile: {p95}\")\n",
    "print(f\"99th percentile: {p99}\")\n",
    "print(f\"Max len: {max_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e83d7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"\n",
    "MODEL_NAME = \"cointegrated/LaBSE-en-ru\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE).eval()\n",
    "model.max_seq_length = 512\n",
    "\n",
    "BATCH_SHORT = 256\n",
    "MAX_MODEL_TOK = 512\n",
    "CHUNK_SIZE = MAX_MODEL_TOK - 2\n",
    "\n",
    "texts = df_with_description[\"embedding_text\"].to_list()\n",
    "n_items = len(texts)\n",
    "\n",
    "\n",
    "def encode_batch(batch_texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"Encode a batch of ≤512‑token texts (no truncation needed).\"\"\"\n",
    "    enc = tokenizer(\n",
    "        batch_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc).last_hidden_state[:, 0]  # CLS\n",
    "    emb = torch.nn.functional.normalize(out, p=2, dim=1)\n",
    "    return emb.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "\n",
    "def encode_long(text: str) -> np.ndarray:\n",
    "    \"\"\"Chunk >512‑token text into windows; mean‑pool CLS embeddings.\"\"\"\n",
    "    # token ids без [CLS]/[SEP]\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    reps = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(ids), CHUNK_SIZE):\n",
    "            chunk_ids = (\n",
    "                [tokenizer.cls_token_id]\n",
    "                + ids[i : i + CHUNK_SIZE]\n",
    "                + [tokenizer.sep_token_id]\n",
    "            )\n",
    "            inp = {\n",
    "                \"input_ids\": torch.tensor([chunk_ids], device=DEVICE),\n",
    "                \"attention_mask\": torch.ones(1, len(chunk_ids), device=DEVICE),\n",
    "            }\n",
    "            h = model(**inp).last_hidden_state[0, 0]  # CLS\n",
    "            reps.append(h.cpu().numpy())\n",
    "    vec = np.mean(reps, axis=0)\n",
    "    vec = vec / np.linalg.norm(vec)\n",
    "    return vec.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1132e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6e586446fc460d8f24b7ff0dcac4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15963 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (534 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.empty((n_items, 768), dtype=\"float32\")\n",
    "\n",
    "short_buffer = []\n",
    "short_idx = []\n",
    "for idx, txt in enumerate(tqdm(texts, total=n_items)):\n",
    "    # быстрый подсчёт токенов (без полной токенизации)\n",
    "    n_tok = len(tokenizer.tokenize(txt))\n",
    "    if n_tok <= CHUNK_SIZE:\n",
    "        short_buffer.append(txt)\n",
    "        short_idx.append(idx)\n",
    "        # если буфер готов отправлять\n",
    "        if len(short_buffer) == BATCH_SHORT:\n",
    "            embeddings[short_idx] = encode_batch(short_buffer)\n",
    "            short_buffer, short_idx = [], []\n",
    "    else:\n",
    "        embeddings[idx] = encode_long(txt)\n",
    "\n",
    "# не забудем хвост\n",
    "if short_buffer:\n",
    "    embeddings[short_idx] = encode_batch(short_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f5c7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
